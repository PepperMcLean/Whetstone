Intro to Big O
  Objectives
    Motivate the need for something like Big O Notation
    Describe what Big O Notation is
    Simplify Big O expressions
  
  Whats the idea here?
    Imagine we have multiple implementations of the same function.
    How can we determine which one is the "best?"
    
  Who cares?
    It's important to have a precise vocabulart to talk about how our code performs
    Useful for discussing trade-offs between different approaches
    When your code slows down or crashes, identifying parts of the code that are inefficient can help us find pain points in our applications
    Less important: it comes up in interviews!

Timing Our code
  An Example (ex.1)
    Suppose we want to write a function that calculates the sum of all numbers from 1 up to (and including) some number n.
  
  What does better mean?
    Faster?
    Less memory-intensive?
    More readable?
    Brevity?
    Frequently the first two are more important than the others
  
  Why not use timers? (ex.1)
  
  The Problem with time
    Different machines will record different times
    The same machine will record different times!
    For fast algorithms, speed measurements may not be precise enough?

  If not time, then what?
    Rather than counting seconds, which are so variable...
    Let's count the number of simple operations the computer has to perform!
      In ex.1 
        V2 has 3 operations
        V1 has (5n + 2) operations
        In V1 regardless of the exact number, the number of operations grows roughly proprtionally with n.

Official Intro to Big O
  Introducing... Big O
    Big O Notation is a way to formalize fuzzy counting
    It allows us to talk formally about how the runtime of an algorithm grows as the inputs grow
    We won't care about the details, only the trends
  Big O Definition
    We say that an algorithm is O(f(n)) if the number of simple operations the computer has to do is 
      eventually less than a constant times f(n), as n increases
    f(n) could be linear (f(n) = n) // O(n)
    f(n) could be quadratic (f(n) = n^2) // O(n^2)
    f(n) could be constant (f(n) = 1) // O(1)
    F(n) could be something entirely different!

Simplifying Big O Expressions
    When determining the time complexity of an algorithm, there are some helpful rule of thumbs for big O expressions.
    These rules of thumb are consquences of the definition of big O notation.

  Constants Don't Matter
    not O(2n), O(n)
    not O(500), O(1)
    not O(13n^2), O(n^2)
  
  Smaller terms Don't Matter

  Big O Shorthands
    Analyzing complexity with big O can get complicated
    There are several rules of thumb that can help
    These rules won't always work, but are a helpful starting point
      1. Arithmetic operations are constant
      2. Variable assignment is constant
      3. Accessing elements in an array (by index) or object (by key) is constant
      4. In a loop, the complexity is the length of the loop times the complexity of whatever happens inside of the loop

Space Complexity
    So far, we've been focusing on time complexity: how can we analyze the runtime of an algorithm as the size of the inputs increases?
    We can also use big O notation to analyze space complexity: how much additional memory do we need to allocate 
      in order to run the code in our algorithm?

  What about the inputs?
    Sometimes you'll hear the term auxiliary space complexity to refer to space required by the algorithm, 
      not including space take up by the inputs.
    Unless otherwise noted, when we talk about space complexity, technically we will be talking about auxiliary space complexity
  
  Space Complexity in JS, Rules of Thumb
    Most primitives (booleans, numbers, undefined, null) are constant space
    Strings require O(n) space (where n is the string length)
    Reference types are generally O(n), where n is the length (for arrays) or the number of keys (for objects)
  
Logarithms
    We've encountere some of the mostcommon complexities: O(1), O(n), O(n^2)
    Sometimes big O expressions involve more complex mathematical expressions
    One that appears more often than you might like is the logarithm!

  Wait, What's a log again?
    Log base2(8) = 3 or 2^3 = 8
    Log base2(value) = exponent, 2^exponent = value
    We care about the big picture so instead of writing log base2, we will write log

  Wut.
    This isn't a math course so here's a rule of thumb.
    The log of a number roughly measures the number of times you can divide that number
     by 2 before you get a value that's less than or equal to one.

  Log Examples
    8/2 = 4/2 = 2/2 = 1 which meants log(8) = 3
    25/2 = 12.5/2 = 6.25/2 = 3.125/2 = 1.5625/2 = 0.78125 which means log(25) = 4.64

  Logarithm Complexity
    Log time complexity is great!
    Compares very favorably to O(n)

  Who Cares?
    Certain searching algorithms have logarithmic time complexity.
    Efficient sorting algorithms involve logarithms.
    Recursion sometimes involves logarithmic space complexity.

Recap
  To analyze performance of an algorithm, we use big O notation
  Big O notation can give us a high level understanding of the time or space complexity of an algorithm
  Big O notation doesn't care about precision, only about general trends (linear? quadratic? constant?)
  The time or space complexity (as measured by big O) depends only on the algorithm, not the hardware used to run the algorithm
  Big O notation is everyhwere, so get lots of practice!
  